name: Sanity Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # Install jq for JSON processing
          sudo apt-get install -y jq
          
      - name: Run tests with Allure
        run: |
          mkdir -p allure-results
          
          # Create collections compatibility module
          cat > collections_compat.py << 'EOF'
          """
          Python 3.10 Compatibility Layer for collections.Mapping
          
          This module provides compatibility for code that relies on collections.Mapping,
          which was removed in Python 3.10 in favor of collections.abc.Mapping.
          """
          
          import collections
          import collections.abc
          import sys
          
          # Apply patches for collections module
          if not hasattr(collections, 'Mapping'):
              collections.Mapping = collections.abc.Mapping
          
          if not hasattr(collections, 'Sequence'):
              collections.Sequence = collections.abc.Sequence
          
          print("Applied compatibility patches for collections module")
          EOF
          
          # Create test runner script that imports compatibility layer first
          cat > run_tests.py << 'EOF'
          import sys
          import os
          
          # Import compatibility layer first
          import collections_compat
          
          # Import pytest and run tests
          import pytest
          
          # Create allure results directory
          if not os.path.exists("allure-results"):
              os.makedirs("allure-results")
          
          # Run pytest with allure reporting
          sys.exit(pytest.main(["tests/", "--alluredir=allure-results"]))
          EOF
          
          # Run tests with our compatibility layer and save output
          python run_tests.py > pytest_output.txt 2>&1
          
          # Check test output for issues
          if [ ! -s pytest_output.txt ]; then
            echo "Warning: Test output file is empty or missing!"
            echo "No test output captured. Tests may have failed to start." > pytest_output.txt
          fi
        continue-on-error: true
        
      - name: Display test output
        if: always()
        run: |
          echo "==== TEST OUTPUT START ===="
          cat pytest_output.txt
          echo "==== TEST OUTPUT END ===="
          
          # Check if the file exists and has content
          if [ -s pytest_output.txt ]; then
            echo "Test output file exists and has content"
          else
            echo "Test output file is missing or empty!"
            # Create a minimal output file for downstream steps
            echo "No test output captured. Tests may have failed to start." > pytest_output.txt
          fi
        
      - name: Store Allure results
        if: always()
        run: |
          zip -r allure-results.zip allure-results/
          echo "Allure results zipped"
          
          # Create a simple HTML summary
          TOTAL=$(find allure-results -name "*.json" | grep -v "environment\|categories\|executor" | wc -l || echo 0)
          PASSED=$(grep -l '"status":"passed"' allure-results/*.json 2>/dev/null | wc -l || echo 0)
          FAILED=$((TOTAL - PASSED))
          
          cat > summary.html << EOF
          <!DOCTYPE html>
          <html>
          <head>
            <title>Test Results Summary</title>
            <style>
              body { font-family: Arial, sans-serif; margin: 20px; }
              .summary { display: flex; gap: 20px; margin-bottom: 20px; }
              .card { padding: 15px; border-radius: 8px; color: white; text-align: center; min-width: 100px; }
              .total { background-color: #2196F3; }
              .passed { background-color: #4CAF50; }
              .failed { background-color: #F44336; }
              .number { font-size: 24px; font-weight: bold; }
              .label { font-size: 14px; }
            </style>
          </head>
          <body>
            <h1>Test Results Summary</h1>
            <div class="summary">
              <div class="card total">
                <div class="number">\${TOTAL}</div>
                <div class="label">Total Tests</div>
              </div>
              <div class="card passed">
                <div class="number">\${PASSED}</div>
                <div class="label">Passed</div>
              </div>
              <div class="card failed">
                <div class="number">\${FAILED}</div>
                <div class="label">Failed</div>
              </div>
            </div>
            <p>Run ID: \${{ github.run_id }}</p>
            <p>Repository: \${{ github.repository }}</p>
            <p>Branch: \${{ github.ref }}</p>
            <p>Download the allure-results.zip artifact for detailed test results.</p>
          </body>
          </html>
          EOF
          
      - name: Upload Allure results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: allure-results
          path: allure-results.zip
          
      - name: Upload Summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: summary.html
          
      - name: Parse test metrics
        if: always()
        run: |
          echo "Parsing test metrics from pytest output..."
          
          # Count collected tests by looking at the collection line
          COLLECTED=$(grep -o "collected [0-9]* item" pytest_output.txt | grep -o "[0-9]*" || echo 0)
          if [ -z "$COLLECTED" ] || [ "$COLLECTED" = "0" ]; then
            # Try alternative pattern
            COLLECTED=$(grep -o "[0-9]* items collected" pytest_output.txt | grep -o "[0-9]*" || echo 0)
          fi
          
          # Count failures and passed tests from summary line
          SUMMARY=$(grep "= .* failed, .* passed" pytest_output.txt || 
                   grep "= .* passed, .* failed" pytest_output.txt || 
                   grep "=* [0-9]* passed" pytest_output.txt || echo "")
          
          if [ -n "$SUMMARY" ]; then
            # Extract failed and passed from summary
            FAILED_TESTS=$(echo "$SUMMARY" | grep -o "[0-9]* failed" | grep -o "[0-9]*" || echo 0)
            PASSED_TESTS=$(echo "$SUMMARY" | grep -o "[0-9]* passed" | grep -o "[0-9]*" || echo 0)
            
            # If we only have "X passed" pattern (all tests passed)
            if [ "$PASSED_TESTS" != "0" ] && [ -z "$FAILED_TESTS" ]; then
              FAILED_TESTS=0
            fi
          else
            # Count them another way - by counting actual FAILED/PASSED lines
            FAILED_TESTS=$(grep -c "FAILED " pytest_output.txt || echo 0)
            # Get total if we have it, otherwise count by pattern
            if [ -n "$COLLECTED" ] && [ "$COLLECTED" != "0" ]; then
              TOTAL_TESTS=$COLLECTED
              PASSED_TESTS=$((TOTAL_TESTS - FAILED_TESTS))
            else
              # Check if no failures means all passed
              if [ "$FAILED_TESTS" = "0" ] && grep -q ".*=+ [0-9]* passed in .*" pytest_output.txt; then
                # All tests passed, extract the number
                PASSED_TESTS=$(grep ".*=+ [0-9]* passed in .*" pytest_output.txt | grep -o "[0-9]* passed" | grep -o "[0-9]*" || echo 0)
                TOTAL_TESTS=$PASSED_TESTS
              else
                # Count .F or .P characters in test progress output
                DOTS_LINE=$(grep -o "\.\.\." pytest_output.txt | head -1 || echo "")
                if [ -n "$DOTS_LINE" ]; then
                  # Count dots and F/P characters as tests
                  TOTAL_TESTS=$(echo "$DOTS_LINE" | wc -c)
                  PASSED_DOTS=$(echo "$DOTS_LINE" | grep -o "\." | wc -l || echo 0)
                  PASSED_TESTS=$PASSED_DOTS
                else
                  # Last resort fallbacks
                  FAILED_TESTS=0
                  PASSED_TESTS=0
                fi
              fi
            fi
          fi
          
          # Set total tests based on passed and failed if not already set
          if [ -z "$TOTAL_TESTS" ] || [ "$TOTAL_TESTS" = "0" ]; then
            TOTAL_TESTS=$((PASSED_TESTS + FAILED_TESTS))
          fi
          
          # Ensure we have valid numbers
          TOTAL_TESTS=${TOTAL_TESTS:-0}
          PASSED_TESTS=${PASSED_TESTS:-0}
          FAILED_TESTS=${FAILED_TESTS:-0}
          
          # Count critical tests by checking for critical_workflow markers in a more precise way
          # Look for actual test definitions or failures of critical tests
          CRITICAL_TESTS=$(grep -E "^(FAILED|PASSED) .*test_critical" pytest_output.txt | wc -l || echo 0)
          if [ "$CRITICAL_TESTS" = "0" ]; then
            # Try alternative approach - look for critical markers with the exact marker
            CRITICAL_TESTS=$(grep -c "@pytest.mark.critical_workflow" pytest_output.txt || echo 0)
          fi
          
          # Sanity check to avoid unreasonable values
          if [ "$CRITICAL_TESTS" -gt "$TOTAL_TESTS" ]; then
            # Can't have more critical tests than total tests
            CRITICAL_TESTS=$TOTAL_TESTS
          fi
          
          # Calculate pass rate
          if [ $TOTAL_TESTS -eq 0 ]; then
            PASS_RATE=0
          else
            PASS_RATE=$(echo "scale=2; ($PASSED_TESTS * 100) / $TOTAL_TESTS" | bc)
          fi
          
          # Format timestamp
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          
          # Create metrics JSON file
          echo "{" > metrics.json
          echo "  \"timestamp\": \"$TIMESTAMP\"," >> metrics.json
          echo "  \"total_tests\": $TOTAL_TESTS," >> metrics.json
          echo "  \"passed_tests\": $PASSED_TESTS," >> metrics.json
          echo "  \"failed_tests\": $FAILED_TESTS," >> metrics.json
          echo "  \"pass_rate\": $PASS_RATE," >> metrics.json
          echo "  \"duration_seconds\": 0," >> metrics.json
          echo "  \"critical_tests_count\": $CRITICAL_TESTS," >> metrics.json
          echo "  \"run_id\": \"${{ github.run_id }}\"," >> metrics.json
          echo "  \"repository\": \"${{ github.repository }}\"," >> metrics.json
          echo "  \"branch\": \"${{ github.ref }}\"" >> metrics.json
          echo "}" >> metrics.json
          
          echo "Created metrics.json with actual test results:"
          cat metrics.json
          
      - name: Upload metrics artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: metrics
          path: metrics.json
          
      - name: Create self-contained dashboard
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          # Read metrics from the metrics.json file
          METRICS_JSON=$(cat metrics.json)
          TOTAL_TESTS=$(echo $METRICS_JSON | jq -r '.total_tests')
          PASSED_TESTS=$(echo $METRICS_JSON | jq -r '.passed_tests')
          FAILED_TESTS=$(echo $METRICS_JSON | jq -r '.failed_tests')
          PASS_RATE=$(echo $METRICS_JSON | jq -r '.pass_rate')
          DURATION=$(echo $METRICS_JSON | jq -r '.duration_seconds')
          CRITICAL_TESTS=$(echo $METRICS_JSON | jq -r '.critical_tests_count')
          
          # Create timestamp
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
          
          # Create a directory for GitHub Pages
          mkdir -p public

          # Copy metrics.json to the public directory
          cp metrics.json public/
          
          # Create a self-contained dashboard HTML file
          cat > public/index.html << EOF
          <!DOCTYPE html>
          <html lang="en">
          <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Sanity Test Metrics Dashboard</title>
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
            <style>
              body { font-family: Arial, sans-serif; margin: 20px; }
              .dashboard { display: flex; flex-wrap: wrap; }
              .chart-container { width: 48%; margin: 1%; height: 300px; }
              .metrics-table { width: 98%; margin: 1%; }
              table { width: 100%; border-collapse: collapse; }
              th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
              th { background-color: #f2f2f2; }
              tr:nth-child(even) { background-color: #f9f9f9; }
              .summary-cards { display: flex; gap: 20px; margin-bottom: 20px; }
              .card { padding: 15px; border-radius: 8px; color: white; text-align: center; min-width: 100px; }
              .total { background-color: #2196F3; }
              .passed { background-color: #4CAF50; }
              .failed { background-color: #F44336; }
              .critical { background-color: #FF9800; }
              .number { font-size: 24px; font-weight: bold; }
              .label { font-size: 14px; }
              @media (max-width: 768px) {
                .chart-container { width: 98%; }
              }
            </style>
          </head>
          <body>
            <h1>Sanity Test Metrics Dashboard</h1>
            
            <div class="summary-cards">
              <div class="card total">
                <div class="number">$TOTAL_TESTS</div>
                <div class="label">Total Tests</div>
              </div>
              <div class="card passed">
                <div class="number">$PASSED_TESTS</div>
                <div class="label">Passed</div>
              </div>
              <div class="card failed">
                <div class="number">$FAILED_TESTS</div>
                <div class="label">Failed</div>
              </div>
              <div class="card critical">
                <div class="number">$CRITICAL_TESTS</div>
                <div class="label">Critical Tests</div>
              </div>
            </div>
            
            <div class="dashboard">
              <div class="chart-container">
                <canvas id="passRateChart"></canvas>
              </div>
              <div class="chart-container">
                <canvas id="testsChart"></canvas>
              </div>
              <div class="metrics-table">
                <h2>Latest Test Run</h2>
                <table>
                  <thead>
                    <tr>
                      <th>Timestamp</th>
                      <th>Total Tests</th>
                      <th>Passed</th>
                      <th>Failed</th>
                      <th>Pass Rate</th>
                      <th>Duration (s)</th>
                      <th>Critical Tests</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>$TIMESTAMP</td>
                      <td>$TOTAL_TESTS</td>
                      <td>$PASSED_TESTS</td>
                      <td>$FAILED_TESTS</td>
                      <td>$PASS_RATE%</td>
                      <td>$DURATION</td>
                      <td>$CRITICAL_TESTS</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
            
            <script>
              // Pass Rate Chart
              new Chart(document.getElementById('passRateChart'), {
                type: 'pie',
                data: {
                  labels: ['Passed', 'Failed'],
                  datasets: [{
                    data: [$PASSED_TESTS, $FAILED_TESTS],
                    backgroundColor: ['#4CAF50', '#F44336']
                  }]
                },
                options: {
                  responsive: true,
                  plugins: {
                    title: {
                      display: true,
                      text: 'Test Pass/Fail Rate'
                    }
                  }
                }
              });
              
              // Tests Chart
              new Chart(document.getElementById('testsChart'), {
                type: 'bar',
                data: {
                  labels: ['Test Results'],
                  datasets: [
                    {
                      label: 'Passed Tests',
                      data: [$PASSED_TESTS],
                      backgroundColor: '#4CAF50'
                    },
                    {
                      label: 'Failed Tests',
                      data: [$FAILED_TESTS],
                      backgroundColor: '#F44336'
                    }
                  ]
                },
                options: {
                  responsive: true,
                  plugins: {
                    title: {
                      display: true,
                      text: 'Test Results'
                    }
                  },
                  scales: {
                    x: {
                      stacked: true
                    },
                    y: {
                      stacked: true
                    }
                  }
                }
              });
            </script>
            
            <footer>
              <p>Last updated: $TIMESTAMP</p>
              <p>Repository: \${{ github.repository }}</p>
              <p>Branch: \${{ github.ref }}</p>
              <p>Run ID: \${{ github.run_id }}</p>
            </footer>
          </body>
          </html>
          EOF
          
          # Create a dashboard.html file (for backward compatibility)
          cp public/index.html public/dashboard.html
          
          # Create a simple README file
          cat > public/README.md << EOF
          # Sanity Test Metrics Dashboard
          
          This dashboard displays metrics from the sanity test pipeline.
          
          ## Available Pages
          
          - [Dashboard](index.html) - Main dashboard with test metrics
          
          ## Latest Test Run
          
          - **Total Tests:** $TOTAL_TESTS
          - **Passed:** $PASSED_TESTS
          - **Failed:** $FAILED_TESTS
          - **Pass Rate:** $PASS_RATE%
          - **Critical Tests:** $CRITICAL_TESTS
          
          Last updated: $TIMESTAMP
          EOF
          
          # List the public directory to verify content
          echo "Public directory content:"
          ls -la public/
          
      - name: Deploy to GitHub Pages
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          force_orphan: true