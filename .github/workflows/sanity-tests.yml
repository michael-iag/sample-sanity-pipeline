name: Sanity Tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # Install jq for JSON processing
          sudo apt-get install -y jq
          
      - name: Run tests with Allure
        run: |
          mkdir -p allure-results
          
          # Create collections compatibility module
          cat > collections_compat.py << EOF
          """
          Python 3.10 Compatibility Layer for collections.Mapping
          
          This module provides compatibility for code that relies on collections.Mapping,
          which was removed in Python 3.10 in favor of collections.abc.Mapping.
          """
          
          import collections
          import collections.abc
          import sys
          
          # Apply patches for collections module
          if not hasattr(collections, 'Mapping'):
              collections.Mapping = collections.abc.Mapping
          
          if not hasattr(collections, 'Sequence'):
              collections.Sequence = collections.abc.Sequence
          
          print("Applied compatibility patches for collections module")
          EOF
          
          # Create test runner script that imports compatibility layer first
          cat > run_tests.py << EOF
          import sys
          import os
          
          # Import compatibility layer first
          import collections_compat
          
          # Import pytest and run tests
          import pytest
          
          # Create allure results directory
          if not os.path.exists("allure-results"):
              os.makedirs("allure-results")
          
          # Run pytest with allure reporting
          sys.exit(pytest.main(["tests/", "--alluredir=allure-results"]))
          EOF
          
          # Run tests with our compatibility layer
          python run_tests.py
        continue-on-error: true
        
      - name: Store Allure results
        if: always()
        run: |
          zip -r allure-results.zip allure-results/
          echo "Allure results zipped"
          
          # Create a simple HTML summary
          TOTAL=$(find allure-results -name "*.json" | grep -v "environment\|categories\|executor" | wc -l || echo 0)
          PASSED=$(grep -l '"status":"passed"' allure-results/*.json 2>/dev/null | wc -l || echo 0)
          FAILED=$((TOTAL - PASSED))
          
          cat > summary.html << EOF
          <!DOCTYPE html>
          <html>
          <head>
            <title>Test Results Summary</title>
            <style>
              body { font-family: Arial, sans-serif; margin: 20px; }
              .summary { display: flex; gap: 20px; margin-bottom: 20px; }
              .card { padding: 15px; border-radius: 8px; color: white; text-align: center; min-width: 100px; }
              .total { background-color: #2196F3; }
              .passed { background-color: #4CAF50; }
              .failed { background-color: #F44336; }
              .number { font-size: 24px; font-weight: bold; }
              .label { font-size: 14px; }
            </style>
          </head>
          <body>
            <h1>Test Results Summary</h1>
            <div class="summary">
              <div class="card total">
                <div class="number">\${TOTAL}</div>
                <div class="label">Total Tests</div>
              </div>
              <div class="card passed">
                <div class="number">\${PASSED}</div>
                <div class="label">Passed</div>
              </div>
              <div class="card failed">
                <div class="number">\${FAILED}</div>
                <div class="label">Failed</div>
              </div>
            </div>
            <p>Run ID: \${{ github.run_id }}</p>
            <p>Repository: \${{ github.repository }}</p>
            <p>Branch: \${{ github.ref }}</p>
            <p>Download the allure-results.zip artifact for detailed test results.</p>
          </body>
          </html>
          EOF
          
      - name: Upload Allure results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: allure-results
          path: allure-results.zip
          
      - name: Upload Summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: summary.html
          
      - name: Parse test metrics
        if: always()
        run: |
          echo "Parsing test metrics..."
          
          # Get test results from Allure JSON files
          TOTAL_TESTS=$(find allure-results -name "*.json" | grep -v "environment\|categories\|executor" | wc -l || echo 0)
          PASSED_TESTS=$(grep -l '"status":"passed"' allure-results/*.json 2>/dev/null | wc -l || echo 0)
          FAILED_TESTS=$((TOTAL_TESTS - PASSED_TESTS))
          
          # Calculate pass rate
          if [ $TOTAL_TESTS -eq 0 ]; then
            PASS_RATE=0
          else
            PASS_RATE=$(echo "scale=2; ($PASSED_TESTS * 100) / $TOTAL_TESTS" | bc)
          fi
          
          # Get critical tests count
          CRITICAL_TESTS=$(grep -l "critical_workflow" allure-results/*.json 2>/dev/null | wc -l || echo 0)
          
          # Get test duration from GitHub Actions timing
          START_TIME=$(date +%s)
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          # Create metrics JSON
          cat > metrics.json << EOF
          {
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "total_tests": $TOTAL_TESTS,
            "passed_tests": $PASSED_TESTS,
            "failed_tests": $FAILED_TESTS,
            "pass_rate": $PASS_RATE,
            "duration_seconds": $DURATION,
            "critical_tests_count": $CRITICAL_TESTS,
            "run_id": "\${{ github.run_id }}",
            "repository": "\${{ github.repository }}",
            "branch": "\${{ github.ref }}"
          }
          EOF
          
          cat metrics.json
          
      - name: Upload metrics artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: metrics
          path: metrics.json
          
      - name: Create self-contained dashboard
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          # Get test results from Allure JSON files
          TOTAL_TESTS=$(find allure-results -name "*.json" | grep -v "environment\|categories\|executor" | wc -l || echo 0)
          PASSED_TESTS=$(grep -l '"status":"passed"' allure-results/*.json 2>/dev/null | wc -l || echo 0)
          FAILED_TESTS=$((TOTAL_TESTS - PASSED_TESTS))
          
          # Calculate pass rate
          if [ $TOTAL_TESTS -eq 0 ]; then
            PASS_RATE=0
          else
            PASS_RATE=$(echo "scale=2; ($PASSED_TESTS * 100) / $TOTAL_TESTS" | bc)
          fi
          
          # Get critical tests count
          CRITICAL_TESTS=$(grep -l "critical_workflow" allure-results/*.json 2>/dev/null | wc -l || echo 0)
          
          # Get test duration from GitHub Actions timing
          START_TIME=$(date +%s)
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          # Create timestamp
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
          
          # Create a directory for GitHub Pages
          mkdir -p public
          
          # Create a self-contained dashboard HTML file
          cat > public/index.html << EOF
          <!DOCTYPE html>
          <html lang="en">
          <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Sanity Test Metrics Dashboard</title>
            <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
            <style>
              body { font-family: Arial, sans-serif; margin: 20px; }
              .dashboard { display: flex; flex-wrap: wrap; }
              .chart-container { width: 48%; margin: 1%; height: 300px; }
              .metrics-table { width: 98%; margin: 1%; }
              table { width: 100%; border-collapse: collapse; }
              th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
              th { background-color: #f2f2f2; }
              tr:nth-child(even) { background-color: #f9f9f9; }
              .summary-cards { display: flex; gap: 20px; margin-bottom: 20px; }
              .card { padding: 15px; border-radius: 8px; color: white; text-align: center; min-width: 100px; }
              .total { background-color: #2196F3; }
              .passed { background-color: #4CAF50; }
              .failed { background-color: #F44336; }
              .critical { background-color: #FF9800; }
              .number { font-size: 24px; font-weight: bold; }
              .label { font-size: 14px; }
              @media (max-width: 768px) {
                .chart-container { width: 98%; }
              }
            </style>
          </head>
          <body>
            <h1>Sanity Test Metrics Dashboard</h1>
            
            <div class="summary-cards">
              <div class="card total">
                <div class="number">$TOTAL_TESTS</div>
                <div class="label">Total Tests</div>
              </div>
              <div class="card passed">
                <div class="number">$PASSED_TESTS</div>
                <div class="label">Passed</div>
              </div>
              <div class="card failed">
                <div class="number">$FAILED_TESTS</div>
                <div class="label">Failed</div>
              </div>
              <div class="card critical">
                <div class="number">$CRITICAL_TESTS</div>
                <div class="label">Critical Tests</div>
              </div>
            </div>
            
            <div class="dashboard">
              <div class="chart-container">
                <canvas id="passRateChart"></canvas>
              </div>
              <div class="chart-container">
                <canvas id="testsChart"></canvas>
              </div>
              <div class="metrics-table">
                <h2>Latest Test Run</h2>
                <table>
                  <thead>
                    <tr>
                      <th>Timestamp</th>
                      <th>Total Tests</th>
                      <th>Passed</th>
                      <th>Failed</th>
                      <th>Pass Rate</th>
                      <th>Duration (s)</th>
                      <th>Critical Tests</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>$TIMESTAMP</td>
                      <td>$TOTAL_TESTS</td>
                      <td>$PASSED_TESTS</td>
                      <td>$FAILED_TESTS</td>
                      <td>$PASS_RATE%</td>
                      <td>$DURATION</td>
                      <td>$CRITICAL_TESTS</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
            
            <script>
              // Pass Rate Chart
              new Chart(document.getElementById('passRateChart'), {
                type: 'pie',
                data: {
                  labels: ['Passed', 'Failed'],
                  datasets: [{
                    data: [$PASSED_TESTS, $FAILED_TESTS],
                    backgroundColor: ['#4CAF50', '#F44336']
                  }]
                },
                options: {
                  responsive: true,
                  plugins: {
                    title: {
                      display: true,
                      text: 'Test Pass/Fail Rate'
                    }
                  }
                }
              });
              
              // Tests Chart
              new Chart(document.getElementById('testsChart'), {
                type: 'bar',
                data: {
                  labels: ['Test Results'],
                  datasets: [
                    {
                      label: 'Passed Tests',
                      data: [$PASSED_TESTS],
                      backgroundColor: '#4CAF50'
                    },
                    {
                      label: 'Failed Tests',
                      data: [$FAILED_TESTS],
                      backgroundColor: '#F44336'
                    }
                  ]
                },
                options: {
                  responsive: true,
                  plugins: {
                    title: {
                      display: true,
                      text: 'Test Results'
                    }
                  },
                  scales: {
                    x: {
                      stacked: true
                    },
                    y: {
                      stacked: true
                    }
                  }
                }
              });
            </script>
            
            <footer>
              <p>Last updated: $TIMESTAMP</p>
              <p>Repository: \${{ github.repository }}</p>
              <p>Branch: \${{ github.ref }}</p>
              <p>Run ID: \${{ github.run_id }}</p>
            </footer>
          </body>
          </html>
          EOF
          
          # Create a dashboard.html file (for backward compatibility)
          cp public/index.html public/dashboard.html
          
          # Create a simple README file
          cat > public/README.md << EOF
          # Sanity Test Metrics Dashboard
          
          This dashboard displays metrics from the sanity test pipeline.
          
          ## Available Pages
          
          - [Dashboard](index.html) - Main dashboard with test metrics
          
          ## Latest Test Run
          
          - **Total Tests:** $TOTAL_TESTS
          - **Passed:** $PASSED_TESTS
          - **Failed:** $FAILED_TESTS
          - **Pass Rate:** $PASS_RATE%
          - **Critical Tests:** $CRITICAL_TESTS
          
          Last updated: $TIMESTAMP
          EOF
          
          # List the public directory to verify content
          echo "Public directory content:"
          ls -la public/
          
      - name: Deploy to GitHub Pages
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          force_orphan: true
